{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers_from_scratch import Transformer\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>Bassa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Il y avait dans le pays d'Uts un homme dont le...</td>\n",
       "      <td>Mut wada a bé yééne i loñ Us, jôl jé li bé le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Et il lui naquit sept fils et trois filles;</td>\n",
       "      <td>A bééna bon bôlôm basaambok, ni bon bôda baa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>et il possédait sept mille brebis, et trois mi...</td>\n",
       "      <td>A bééna ki 7 000 di mintômba, 3 000 di kamél, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Et ses fils allaient et faisaient un festin, c...</td>\n",
       "      <td>Hiki man wé nu munlôm a bééna yé ngéda i tégba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Et il arrivait que, quand les jours de festin ...</td>\n",
       "      <td>I ngéda ba bé ba mal i mangand ma, Hiôb a bé a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              French  \\\n",
       "0  Il y avait dans le pays d'Uts un homme dont le...   \n",
       "1        Et il lui naquit sept fils et trois filles;   \n",
       "2  et il possédait sept mille brebis, et trois mi...   \n",
       "3  Et ses fils allaient et faisaient un festin, c...   \n",
       "4  Et il arrivait que, quand les jours de festin ...   \n",
       "\n",
       "                                               Bassa  \n",
       "0  Mut wada a bé yééne i loñ Us, jôl jé li bé le ...  \n",
       "1      A bééna bon bôlôm basaambok, ni bon bôda baa.  \n",
       "2  A bééna ki 7 000 di mintômba, 3 000 di kamél, ...  \n",
       "3  Hiki man wé nu munlôm a bééna yé ngéda i tégba...  \n",
       "4  I ngéda ba bé ba mal i mangand ma, Hiôb a bé a...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_path = \"src/sample_dataset_1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to train an encoder-decoder transformer model from scratch where the encoder processes French text and the decoder processes Bassa text. To do this, it is better to use separate tokenizers for each language. This way, each tokenizer can focus on the specific vocabulary and tokenization rules of its respective language, which can improve the efficiency and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Separate Tokenizers\n",
    "We'll create separate tokenizers for French and Bassa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the French texts to a temporary file\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "french_texts_path = \"temp/french_texts.txt\"\n",
    "data['French'].to_csv(french_texts_path, index=False, header=False)\n",
    "\n",
    "# Save the Bassa texts to a temporary file\n",
    "bassa_texts_path = \"temp/bassa_texts.txt\"\n",
    "data['Bassa'].to_csv(bassa_texts_path, index=False, header=False)\n",
    "\n",
    "# Initialize and train the French tokenizer\n",
    "french_tokenizer = BertWordPieceTokenizer()\n",
    "french_tokenizer.train(files=[french_texts_path], vocab_size=30_000, min_frequency=2, special_tokens=[\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "])\n",
    "os.makedirs(\"french_tokenizer\", exist_ok=True)\n",
    "french_tokenizer.save_model(\"french_tokenizer\")\n",
    "\n",
    "# Initialize and train the Bassa tokenizer\n",
    "bassa_tokenizer = BertWordPieceTokenizer()\n",
    "bassa_tokenizer.train(files=[bassa_texts_path], vocab_size=30_000, min_frequency=2, special_tokens=[\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "])\n",
    "os.makedirs(\"bassa_tokenizer\", exist_ok=True)\n",
    "bassa_tokenizer.save_model(\"bassa_tokenizer\")\n",
    "\n",
    "# Load the tokenizers using the transformers library\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "french_tokenizer = BertTokenizerFast.from_pretrained(\"french_tokenizer\")\n",
    "bassa_tokenizer = BertTokenizerFast.from_pretrained(\"bassa_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Dataset\n",
    "\n",
    "We will prepare the datasets for training using the respective tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_french, val_french, train_bassa, val_bassa = train_test_split(\n",
    "    data['French'], data['Bassa'], test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare dataset\n",
    "def tokenize_and_prepare_dataset(src_texts, tgt_texts, src_tokenizer, tgt_tokenizer):\n",
    "    src_encodings = src_tokenizer(src_texts.tolist(), truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
    "    tgt_encodings = tgt_tokenizer(tgt_texts.tolist(), truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
    "\n",
    "    dataset = TensorDataset(src_encodings['input_ids'], tgt_encodings['input_ids'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15606 [09:35<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare training and validation datasets\n",
    "train_dataset = tokenize_and_prepare_dataset(train_french, train_bassa, french_tokenizer, bassa_tokenizer)\n",
    "val_dataset = tokenize_and_prepare_dataset(val_french, val_bassa, french_tokenizer, bassa_tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the Model\n",
    "Now, we need to set up the training loop to train your custom transformer model using the prepared data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "src_vocab_size = french_tokenizer.vocab_size\n",
    "trg_vocab_size = bassa_tokenizer.vocab_size\n",
    "# Define the padding token indices for the source and target tokenizers\n",
    "src_pad_idx = french_tokenizer.pad_token_id\n",
    "trg_pad_idx = bassa_tokenizer.pad_token_id\n",
    "embed_size = 256\n",
    "num_layers = 6\n",
    "forward_expansion = 4\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "max_length = 100\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# Initialize the transformer model\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx=src_pad_idx, trg_pad_idx=trg_pad_idx, embed_size=embed_size, num_layers=num_layers, forward_expansion=forward_expansion, heads=heads, dropout=dropout, device=device, max_length=max_length).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/651], Loss: 9.0890\n",
      "Epoch [1/10], Step [100/651], Loss: 5.5956\n",
      "Epoch [1/10], Step [200/651], Loss: 5.6855\n",
      "Epoch [1/10], Step [300/651], Loss: 5.6484\n",
      "Epoch [1/10], Step [400/651], Loss: 5.7071\n",
      "Epoch [1/10], Step [500/651], Loss: 5.0472\n",
      "Epoch [1/10], Step [600/651], Loss: 4.9304\n",
      "Validation Loss after Epoch [1/10]: 4.8212\n",
      "Epoch [2/10], Step [0/651], Loss: 4.8757\n",
      "Epoch [2/10], Step [100/651], Loss: 4.8275\n",
      "Epoch [2/10], Step [200/651], Loss: 4.7205\n",
      "Epoch [2/10], Step [300/651], Loss: 4.4361\n",
      "Epoch [2/10], Step [400/651], Loss: 4.5861\n",
      "Epoch [2/10], Step [500/651], Loss: 4.5293\n",
      "Epoch [2/10], Step [600/651], Loss: 4.4801\n",
      "Validation Loss after Epoch [2/10]: 4.3943\n",
      "Epoch [3/10], Step [0/651], Loss: 4.5319\n",
      "Epoch [3/10], Step [100/651], Loss: 4.4030\n",
      "Epoch [3/10], Step [200/651], Loss: 4.4922\n",
      "Epoch [3/10], Step [300/651], Loss: 4.3558\n",
      "Epoch [3/10], Step [400/651], Loss: 4.2479\n",
      "Epoch [3/10], Step [500/651], Loss: 4.4492\n",
      "Epoch [3/10], Step [600/651], Loss: 4.1903\n",
      "Validation Loss after Epoch [3/10]: 4.1169\n",
      "Epoch [4/10], Step [0/651], Loss: 4.1512\n",
      "Epoch [4/10], Step [100/651], Loss: 4.1912\n",
      "Epoch [4/10], Step [200/651], Loss: 4.1230\n",
      "Epoch [4/10], Step [300/651], Loss: 4.1216\n",
      "Epoch [4/10], Step [400/651], Loss: 4.2506\n",
      "Epoch [4/10], Step [500/651], Loss: 4.1222\n",
      "Epoch [4/10], Step [600/651], Loss: 3.9457\n",
      "Validation Loss after Epoch [4/10]: 3.9278\n",
      "Epoch [5/10], Step [0/651], Loss: 3.8623\n",
      "Epoch [5/10], Step [100/651], Loss: 3.9942\n",
      "Epoch [5/10], Step [200/651], Loss: 3.9859\n",
      "Epoch [5/10], Step [300/651], Loss: 3.9243\n",
      "Epoch [5/10], Step [400/651], Loss: 3.8464\n",
      "Epoch [5/10], Step [500/651], Loss: 3.7763\n",
      "Epoch [5/10], Step [600/651], Loss: 3.9338\n",
      "Validation Loss after Epoch [5/10]: 3.7537\n",
      "Epoch [6/10], Step [0/651], Loss: 3.9797\n",
      "Epoch [6/10], Step [100/651], Loss: 3.7048\n",
      "Epoch [6/10], Step [200/651], Loss: 3.6266\n",
      "Epoch [6/10], Step [300/651], Loss: 3.8566\n",
      "Epoch [6/10], Step [400/651], Loss: 3.8599\n",
      "Epoch [6/10], Step [500/651], Loss: 3.6463\n",
      "Epoch [6/10], Step [600/651], Loss: 3.6882\n",
      "Validation Loss after Epoch [6/10]: 3.6587\n",
      "Epoch [7/10], Step [0/651], Loss: 3.5803\n",
      "Epoch [7/10], Step [100/651], Loss: 3.6939\n",
      "Epoch [7/10], Step [200/651], Loss: 3.5623\n",
      "Epoch [7/10], Step [300/651], Loss: 3.6328\n",
      "Epoch [7/10], Step [400/651], Loss: 3.5489\n",
      "Epoch [7/10], Step [500/651], Loss: 3.6519\n",
      "Epoch [7/10], Step [600/651], Loss: 3.8003\n",
      "Validation Loss after Epoch [7/10]: 3.5302\n",
      "Epoch [8/10], Step [0/651], Loss: 3.5727\n",
      "Epoch [8/10], Step [100/651], Loss: 3.5475\n",
      "Epoch [8/10], Step [200/651], Loss: 3.4670\n",
      "Epoch [8/10], Step [300/651], Loss: 3.5700\n",
      "Epoch [8/10], Step [400/651], Loss: 3.7473\n",
      "Epoch [8/10], Step [500/651], Loss: 3.7169\n",
      "Epoch [8/10], Step [600/651], Loss: 3.5207\n",
      "Validation Loss after Epoch [8/10]: 3.4612\n",
      "Epoch [9/10], Step [0/651], Loss: 3.4869\n",
      "Epoch [9/10], Step [100/651], Loss: 3.3892\n",
      "Epoch [9/10], Step [200/651], Loss: 3.5862\n",
      "Epoch [9/10], Step [300/651], Loss: 3.2420\n",
      "Epoch [9/10], Step [400/651], Loss: 3.3699\n",
      "Epoch [9/10], Step [500/651], Loss: 3.4551\n",
      "Epoch [9/10], Step [600/651], Loss: 3.4530\n",
      "Validation Loss after Epoch [9/10]: 3.3626\n",
      "Epoch [10/10], Step [0/651], Loss: 3.3880\n",
      "Epoch [10/10], Step [100/651], Loss: 3.4382\n",
      "Epoch [10/10], Step [200/651], Loss: 3.3022\n",
      "Epoch [10/10], Step [300/651], Loss: 3.3927\n",
      "Epoch [10/10], Step [400/651], Loss: 3.4305\n",
      "Epoch [10/10], Step [500/651], Loss: 3.4721\n",
      "Epoch [10/10], Step [600/651], Loss: 3.2605\n",
      "Validation Loss after Epoch [10/10]: 3.3210\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        trg_input = trg[:, :-1]\n",
    "        trg_target = trg[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg_input)\n",
    "        \n",
    "        # Reshape output and target to calculate loss\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        trg_target = trg_target.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_target = trg[:, 1:]\n",
    "\n",
    "            output = model(src, trg_input)\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            trg_target = trg_target.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg_target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss after Epoch [{epoch+1}/{num_epochs}]: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bassa_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = french_tokenizer.pad_token_id\n",
    "trg_pad_idx = bassa_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_model.pth\n",
      "Model loaded from transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_save_path = \"transformer_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0.1, device=device, max_length=100).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    src_tokens = src_tokenizer(sentence, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    src_tokens = src_tokens['input_ids'].to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate the source mask\n",
    "        src_mask = model.make_src_mask(src_tokens)\n",
    "        \n",
    "        # Encode the source tokens\n",
    "        enc_src = model.encoder(src_tokens, src_mask)\n",
    "        \n",
    "        # Prepare the initial target input token ([CLS] token)\n",
    "        tgt_tokens = torch.tensor([[tgt_tokenizer.cls_token_id]], dtype=torch.long).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Generate the target mask\n",
    "            trg_mask = model.make_trg_mask(tgt_tokens).to(device)\n",
    "            \n",
    "            # Decode the current target tokens\n",
    "            output = model.decoder(tgt_tokens, enc_src, src_mask, trg_mask)\n",
    "            \n",
    "            # Get the last token's logits and apply softmax to get probabilities\n",
    "            preds = output[:, -1, :].softmax(dim=-1)\n",
    "            \n",
    "            # Get the token ID with the highest probability\n",
    "            next_token = preds.argmax(1).unsqueeze(0)\n",
    "            \n",
    "            # Concatenate the predicted token to the target tokens\n",
    "            tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "            \n",
    "            # Stop if the end token is generated\n",
    "            if next_token.item() == tgt_tokenizer.sep_token_id:\n",
    "                break\n",
    "        \n",
    "    # Decode the token IDs to get the translated sentence\n",
    "    translated_sentence = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: yehova a podos ki me, a kal nye le :\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming the model and tokenizers are already initialized and trained\n",
    "# sentence = \"Your input sentence in French\"\n",
    "#sentence = \"Bonjour tout le monde\"\n",
    "sentence = \"Et l'Éternel dit à Satan\"\n",
    "translated_sentence = translate_sentence(loaded_model, sentence, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: yehova a bi podos mosi, a kal nye le :\n"
     ]
    }
   ],
   "source": [
    "sentence = \"l'Éternel\"\n",
    "translated_sentence = translate_sentence(loaded_model, sentence, french_tokenizer, bassa_tokenizer, device)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation\n",
    "\n",
    "1. **Tokenizing the Dataset**:\n",
    "   - **Splitting the Dataset**:\n",
    "     - We use `train_test_split` to split the dataset into training and validation sets.\n",
    "     - This helps in evaluating the model on unseen data to understand its performance better.\n",
    "   - **Tokenizing the Text**:\n",
    "     - We use the `french_tokenizer` and `bassa_tokenizer` to convert the French and Bassa texts into token IDs.\n",
    "     - Token IDs are necessary for the model to process the text.\n",
    "\n",
    "2. **Creating the Custom Transformer Model**:\n",
    "   - **Model Components**:\n",
    "     - `SelfAttention`: Computes attention scores and applies them to the values.\n",
    "     - `TransformerBlock`: Consists of self-attention and feed-forward network, with layer normalization and dropout.\n",
    "     - `Encoder`: Stacks multiple `TransformerBlock`s and adds positional encoding to the input embeddings.\n",
    "     - `DecoderBlock`: Similar to `TransformerBlock` but includes an additional attention layer for encoder-decoder attention.\n",
    "     - `Decoder`: Stacks multiple `DecoderBlock`s and adds positional encoding to the input embeddings, followed by a linear layer to produce output token logits.\n",
    "     - `Transformer`: Combines `Encoder` and `Decoder`, applying masks to the source and target sequences during forward passes.\n",
    "\n",
    "3. **Training the Model**:\n",
    "   - **Preparing the DataLoaders**:\n",
    "     - `DataLoader` is used to batch the data and shuffle it during training.\n",
    "     - This helps in efficient data loading and training stability.\n",
    "   - **Training Loop**:\n",
    "     - For each epoch, we iterate over batches of the training data.\n",
    "     - For each batch, we perform a forward pass, compute the loss, perform backpropagation, and update the model weights.\n",
    "     - We evaluate the model on the validation set at the end of each epoch to track its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
